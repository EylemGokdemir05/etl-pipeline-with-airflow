# ETL Pipeline with Airflow & dbt

A productionâ€‘oriented EL(T) pipeline orchestrated with **Apache Airflow** and **dbt (Data Build Tool)** using **Docker** & **Dockerâ€‘inâ€‘Docker (dind)**.

This repository showcases how to operationalize dbt data transformations inside Airflow DAGs by leveraging containerization and clean environment isolation.

---

## ğŸš€ Project Overview

This project defines a modern EL(T) workflow using:

- **Airflow** to schedule and orchestrate jobs,
- **DockerOperator** to run dbt inside containers,
- **dbt** to build, test, and document analytics models,
- **Dockerâ€‘inâ€‘Docker (dind)** to enable Airflow containers to execute other containerized tasks safely.

The goal is to produce tested and documented models in an automated manner while keeping your repository clean and reproducible.

---

## ğŸ§± Architecture

```text
Airflow Scheduler
      â†“
DockerOperator runs dbt deps â†’ dbt build â†’ dbt docs
      â†“
Generated docs copied to shared volume
```


## ğŸ›  Workflow Steps
### 1ï¸âƒ£ Install Dependencies

```text

pip install -r requirements.txt

```
or bring up Airflow via Docker:
```text

docker-compose up -d

```
### 2ï¸âƒ£ Access Airflow UI
Navigate to:
```text

http://localhost:8080

```
Trigger the DAG named:
```text

dbt_build_with_dockeroperator

```
